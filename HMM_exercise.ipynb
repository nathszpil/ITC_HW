{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0W3vlijxKLd"
      },
      "source": [
        "# HMM Exercise\n",
        "\n",
        "In this exercise, we will use the ``hmmlearn`` library to practice what we have learned about Hidden Markov Models (HMMs), both on a toy problem and to build a POS tagger from scratch. First make sure that ``hmmlearn`` is installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_sxxN-m7LK2",
        "outputId": "27a36fb2-a926-4bee-c57e-13123bf97500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install hmmlearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThDA-4eSH-Tr"
      },
      "source": [
        "## Part 1: Seasonal weather\n",
        "\n",
        "In this section, we will consider the following toy problem:\n",
        "\n",
        "A year has four seasons - winter, spring, summer, and fall. Every day of the year we go outside, look at the sky to see if it is sunny or rainy, and try to guess what season it is.\n",
        "\n",
        "**Questions:**\n",
        "1. If we model this with an HMM, what are the hidden states? What are the observed variables?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kcPFZJzl5Js"
      },
      "source": [
        "Hidden states represent the seasons (Winter, Spring, Summer, Fall).\n",
        "Observed variables represent the weather conditions (Sunny, Rainy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjHVVP1Pl5Jt"
      },
      "source": [
        "2. Using ``hmmlearn.hmm.CategoricalHMM``, create an HMM model for this problem. Note: you may refer to [the hmmlearn API documentation](https://hmmlearn.readthedocs.io/en/latest/api.html).\n",
        "  \n",
        "  * Instantiate the HMM model with ``weather_model = CategoricalHMM(n_components=...)``, where ``n_components`` is set to the number of hidden states.\n",
        "  </br>\n",
        "  \n",
        "  * Set ``weather_model.startprob_`` to be uniform (all hidden states with equal probability)\n",
        "      </br>Hint: ``weather_model.startprob_`` should be a list of numbers.\n",
        "  </br>\n",
        "  \n",
        "  * Set ``weather_model.transmat_`` so that on every day, there is a 0.99 probability of the season remaining the same as the previous day and a 0.01 probability of it being the next season.\n",
        "      </br>Hint: ``model.transmat_`` should be a NumPy array of shape ``(n_components, n_components)``\n",
        "  </br>\n",
        "  * Set ``weather_model.emissionprob_`` so that 90% of summer days are sunny, 90% of winter days are rainy, and 50% of spring and fall days are sunny or rainy.\n",
        "      </br>Hint: ``model.emissionprob_`` should be a NumPy array of shape ``(n_components, n_features)``.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "JsFo9qir7GEJ"
      },
      "outputs": [],
      "source": [
        "from hmmlearn import hmm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "9JoRJm6ml5Jt"
      },
      "outputs": [],
      "source": [
        "n_states = 4\n",
        "weather_model = hmm.CategoricalHMM(n_components=n_states)\n",
        "\n",
        "weather_model.startprob_ = np.ones(n_states) / n_states\n",
        "\n",
        "transition_prob_same_season = 0.99\n",
        "transition_prob_next_season = 0.01\n",
        "\n",
        "transmat = np.zeros((n_states, n_states))\n",
        "for i in range(n_states):\n",
        "    transmat[i, i] = transition_prob_same_season\n",
        "    transmat[i, (i + 1) % n_states] = transition_prob_next_season\n",
        "\n",
        "weather_model.transmat_ = transmat\n",
        "\n",
        "emissionprob = np.array([[0.1, 0.9], # Summer\n",
        "                         [0.5, 0.5], # Fall\n",
        "                         [0.9, 0.1], # Winter\n",
        "                         [0.5, 0.5]]) # Spring\n",
        "\n",
        "weather_model.emissionprob_ = emissionprob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT-P_F68l5Jt"
      },
      "source": [
        "3. Sample 100 days from the model by using  ``weather_model.sample(100)``. How many sunny days are observed? How many times did the season change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gozz7-KPl5Ju",
        "outputId": "8129ef30-c1a8-4bfd-b03d-905874f143de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sunny days observed: 55\n",
            "Number of times the season changed: 0\n"
          ]
        }
      ],
      "source": [
        "weather_samples, sampled_states  = weather_model.sample(100)\n",
        "\n",
        "num_sunny_days = np.sum(weather_samples == 1)\n",
        "\n",
        "num_season_changes = 0\n",
        "for i in range(len(sampled_states) - 1) :\n",
        "    if sampled_states[i] != sampled_states[i+1]:\n",
        "        num_season_changes += 1\n",
        "\n",
        "print(\"Number of sunny days observed:\", num_sunny_days)\n",
        "print(\"Number of times the season changed:\", num_season_changes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHhwaJsPl5Ju"
      },
      "source": [
        "4. If 50 sunny days and then 50 rainy days are observed, what is the most likely sequence of seasons for those days under this model? Use ``weather_model.decode(...)`` to determine this.\n",
        "</br>Hint: The input to this function should be a NumPy array of shape ``(100, 1)``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YLtXGOPl5Ju",
        "outputId": "6cf587f1-7f46-4463-f90f-04546c6310ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
          ]
        }
      ],
      "source": [
        "observed_sequence = np.concatenate((np.ones((50, 1), dtype=int), np.zeros((50, 1), dtype=int)), axis=0)\n",
        "_ , predicted_states = weather_model.decode(observed_sequence)\n",
        "\n",
        "print(predicted_states)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddTB2_igH0vg"
      },
      "source": [
        "## Part 2: Building a POS tagger\n",
        "\n",
        "In this section, we will build a simple Part-of-Speech (POS) tagger for English texts based on labelled data from the [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus). First make sure that the necessary corpora are downloaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luRAKwiNP1pn",
        "outputId": "ed7ccaa3-c81f-4ad1-beb6-4605099e34e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtsiB6X2p7-z"
      },
      "source": [
        "Now we will build a HMM-based POS tagger from scratch, step-by-step:\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "5. Get POS-tagged sentences from the Brown corpus using ``nltk.corpus.brown.tagged_sents(tagset='universal')``. Use ``sklearn.model_selection.train_test_split`` to split them into 80% training data and 20% testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7fW1EfQl5Jv",
        "outputId": "31ac3ee7-72a1-46dd-fea5-021ac3e45627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 45872\n",
            "Number of testing sentences: 11468\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tagged_sentences = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
        "\n",
        "train_data, test_data = train_test_split(tagged_sentences, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Number of training sentences:\", len(train_data))\n",
        "print(\"Number of testing sentences:\", len(test_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample sentences from the training data:\")\n",
        "for i in range(5):\n",
        "    print(train_data[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLYh-rmKm-qj",
        "outputId": "f18849c5-b7db-416e-ebff-f8d003713983"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample sentences from the training data:\n",
            "[('frontiers', 'NOUN'), ('.', '.')]\n",
            "[('The', 'DET'), ('pastor', 'NOUN'), ('calls', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('home', 'NOUN'), ('of', 'ADP'), ('each', 'DET'), ('individual', 'NOUN'), ('or', 'CONJ'), ('family', 'NOUN'), ('for', 'ADP'), ('a', 'DET'), ('spiritual', 'ADJ'), ('guidance', 'NOUN'), ('conference', 'NOUN'), ('.', '.')]\n",
            "[('``', '.'), ('Standard', 'ADJ'), ('deal', 'NOUN'), (',', '.'), ('Mr.', 'NOUN'), ('Skyros', 'NOUN'), ('.', '.')]\n",
            "[('Mr.', 'NOUN'), ('Mills', 'NOUN'), ('had', 'VERB'), ('done', 'VERB'), ('some', 'DET'), ('figuring', 'NOUN'), ('on', 'ADP'), ('a', 'DET'), ('scrap', 'NOUN'), ('of', 'ADP'), ('paper', 'NOUN'), ('and', 'CONJ'), ('given', 'VERB'), ('him', 'PRON'), ('the', 'DET'), ('various', 'ADJ'), ('kinds', 'NOUN'), ('of', 'ADP'), ('boards', 'NOUN'), ('and', 'CONJ'), ('two-by-fours', 'NOUN'), ('which', 'DET'), (',', '.'), ('properly', 'ADV'), ('handled', 'VERB'), (',', '.'), ('would', 'VERB'), (',', '.'), ('he', 'PRON'), ('had', 'VERB'), ('assured', 'VERB'), ('him', 'PRON'), (',', '.'), ('turn', 'VERB'), ('into', 'ADP'), ('a', 'DET'), ('workbench', 'NOUN'), ('.', '.')]\n",
            "[('Dazed', 'VERB'), (',', '.'), ('Phil', 'NOUN'), ('said', 'VERB'), (':', '.'), ('``', '.'), ('I', 'PRON'), (\"don't\", 'VERB'), ('get', 'VERB'), ('it', 'PRON'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz9doLkCl5Jv"
      },
      "source": [
        "6. Define ``POS_TAGS`` to be an array of all unique POS tags found in the training data, and define `N_TAGS` to be the number of unique POS tags in this array. What is the value of `N_TAGS`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTZFDN7Rl5Jv",
        "outputId": "9486cac0-e910-4149-bc1b-1c2000c559d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique POS tags: 12\n",
            "{'NOUN', 'DET', 'PRT', 'ADV', 'PRON', 'ADJ', 'X', '.', 'VERB', 'NUM', 'CONJ', 'ADP'}\n"
          ]
        }
      ],
      "source": [
        "POS_TAGS = set(tag for sentence in train_data for word, tag in sentence)\n",
        "N_TAGS = len(POS_TAGS)\n",
        "\n",
        "print(\"Number of unique POS tags:\", N_TAGS)\n",
        "print(POS_TAGS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWzlmHmtl5Jw"
      },
      "source": [
        "7. Using `collections.Counter`, find the 5000 most common word tokens in the training data, and save their unique values to an array ``vocab``. Make all words lowercase before counting, and in addition, add the token '[UNK]' as the first element of ``vocab`` (representing words which are \"out of vocabulary\"). Hint: the first five elements of ``vocab`` should be \\[\"\\[UNK\\]\", \"the\", \",\", \".\", \"of\", ...\\]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXMUPNjKl5Jw",
        "outputId": "40b467d1-64a6-4e0e-a1cd-57106735f32e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First five elements of vocab: ['[UNK]', 'the', ',', '.', 'of']\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_tokens = [word.lower() for sentence in train_data for word, _ in sentence]\n",
        "word_counts = Counter(word_tokens)\n",
        "vocab = [\"[UNK]\"] + [word for word, _ in word_counts.most_common(4999)]\n",
        "\n",
        "print(\"First five elements of vocab:\", vocab[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90hvTVyyl5Jw"
      },
      "source": [
        "8. Create an array `S` of start probabilities for each POS tag (i.e. what fraction of sentences start with a noun? What fraction of them start with a verb? Etc.) Initialize `S = np.zeros(N_TAGS)` and then iterate through the sentences in the training set and add one to an element of `S` for each sentence. Finally set `S = S / sum(S)` to get probabilities.  \n",
        "</br>*Note: This step should take less than 1 second to run (use tqdm to track its progress).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMH7l7qvl5Jw",
        "outputId": "80bfb06a-57f2-4b7a-c71e-6b9890da18c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating start probabilities: 100%|██████████| 45872/45872 [00:00<00:00, 283666.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Start probabilities: [0.14093565 0.21300576 0.03705964 0.09016393 0.16025026 0.03396407\n",
            " 0.0003924  0.08918294 0.04606296 0.01676404 0.04843913 0.12377921]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "S = np.zeros(N_TAGS)\n",
        "\n",
        "for sentence in tqdm(train_data, desc=\"Calculating start probabilities\"):\n",
        "    first_tag = sentence[0][1]\n",
        "    S[list(POS_TAGS).index(first_tag)] += 1\n",
        "\n",
        "S /= np.sum(S)\n",
        "\n",
        "print(\"\\n\\n Start probabilities:\", S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BftAM3bbl5Jw"
      },
      "source": [
        "9. Create a matrix `P` of transition probabilities of transitioning from one POS tag to another, estimated from the training data.  Initialize a matrix `P = np.zeros(shape=(N_TAGS, N_TAGS))`. Then iterate through each pair of words in the training dataset and add one to an element of `P` for each such pair. Finally set `P /= P.sum(axis=1)[:, None]` to get probabilities.\n",
        "</br>*Note: This step should take less than 5 seconds to run (use tqdm to track its progress).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlaNs12ul5Jw",
        "outputId": "7f7eb406-b231-4dd6-b3fa-54693aa8c81f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating transition probabilities: 100%|██████████| 45872/45872 [00:01<00:00, 24213.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transition probabilities matrix:\n",
            "[[1.50334759e-01 1.54993659e-02 1.79719921e-02 2.64398275e-02\n",
            "  1.96855583e-02 1.27858406e-02 3.63621488e-04 2.84742897e-01\n",
            "  1.58248072e-01 8.00421801e-03 6.00520888e-02 2.45871760e-01]\n",
            " [6.26169760e-01 5.93439300e-03 2.08160247e-03 1.75840630e-02\n",
            "  9.85109238e-03 2.39959464e-01 1.36947531e-03 1.26083027e-02\n",
            "  6.47853119e-02 9.85109238e-03 6.02569136e-04 9.20287407e-03]\n",
            " [3.67171528e-02 8.34063504e-02 1.18496266e-02 3.54654316e-02\n",
            "  7.21825844e-03 1.86923687e-02 0.00000000e+00 7.68974006e-02\n",
            "  6.21729879e-01 4.96516043e-03 1.18913506e-02 9.11670213e-02]\n",
            " [3.28587319e-02 7.35483871e-02 2.85428254e-02 9.71523915e-02\n",
            "  4.73637375e-02 1.36618465e-01 8.89877642e-05 1.70322581e-01\n",
            "  2.40200222e-01 1.33036707e-02 1.76195773e-02 1.42380423e-01]\n",
            " [8.80800081e-03 1.72352523e-02 2.41648898e-02 5.37364199e-02\n",
            "  8.60493451e-03 9.44258300e-03 2.53832876e-05 1.03183064e-01\n",
            "  7.07203777e-01 1.14224794e-03 1.13717129e-02 5.50817342e-02]\n",
            " [6.53532365e-01 5.97692905e-03 1.93801925e-02 9.74239436e-03\n",
            "  3.98960014e-03 5.68555376e-02 4.63212002e-04 9.94411571e-02\n",
            "  1.73629789e-02 7.06771861e-03 3.82672883e-02 8.79206264e-02]\n",
            " [5.49242424e-02 6.62878788e-03 6.62878788e-03 7.57575758e-03\n",
            "  6.62878788e-03 1.89393939e-03 5.05681818e-01 2.70833333e-01\n",
            "  5.96590909e-02 9.46969697e-04 2.27272727e-02 5.58712121e-02]\n",
            " [1.33543940e-01 1.09269440e-01 2.98542162e-02 7.01840759e-02\n",
            "  7.44098903e-02 4.66343918e-02 1.88725691e-03 1.72971199e-01\n",
            "  1.24230738e-01 1.95973852e-02 1.12264435e-01 1.05153032e-01]\n",
            " [9.78208730e-02 1.63043925e-01 6.57369972e-02 1.03501679e-01\n",
            "  5.52114027e-02 5.74864661e-02 1.78167615e-04 8.06002878e-02\n",
            "  1.83471527e-01 9.10710615e-03 1.44658398e-02 1.69375728e-01]\n",
            " [3.83089595e-01 1.35316860e-02 5.54715078e-03 2.00033619e-02\n",
            "  8.32072617e-03 5.89174651e-02 2.52143217e-04 2.71558245e-01\n",
            "  4.53017314e-02 2.16002690e-02 3.97545806e-02 1.32123046e-01]\n",
            " [2.42553886e-01 1.50587851e-01 2.50489876e-02 9.19660353e-02\n",
            "  6.72109732e-02 1.13324624e-01 4.57217505e-04 2.09666884e-02\n",
            "  1.95460483e-01 1.93011104e-02 2.28608752e-04 7.28935336e-02]\n",
            " [2.59393051e-01 4.54935770e-01 1.43288215e-02 1.55616864e-02\n",
            "  6.95663419e-02 8.23950341e-02 5.00043107e-04 9.81981205e-03\n",
            "  4.15121993e-02 3.02439866e-02 1.82774377e-03 1.99155100e-02]]\n",
            "(12, 12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "P = np.zeros(shape=(N_TAGS, N_TAGS))\n",
        "\n",
        "for sentence in tqdm(train_data, desc=\"Calculating transition probabilities\"):\n",
        "    for i in range(len(sentence) - 1):\n",
        "        tag1 = sentence[i][1]\n",
        "        tag2 = sentence[i + 1][1]\n",
        "        P[list(POS_TAGS).index(tag1), list(POS_TAGS).index(tag2)] += 1\n",
        "\n",
        "P /= P.sum(axis=1)[:, None]\n",
        "\n",
        "print(\"Transition probabilities matrix:\")\n",
        "print(P)\n",
        "print(P.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ci8byBDl5Jw"
      },
      "source": [
        "10. Create a matrix `E` of emission probabilities, the probability of outputting each word in `vocab` for a given POS tag. Initialize `E = np.zeros(shape=(N_TAGS, len(vocab)))`. Then iterate through each word and POS tag in the training dataset. Make each word lowercase and replace words not in `vocab` with \"\\[UNK\\]\". For each word and POS tag, add one to some element of `E`. Finally set `E /= E.sum(axis=1)[:, None]` to get probabilities.\n",
        "</br>*Note: This step should take approximately 10 seconds to run (use tqdm to track its progress).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlEhtB31l5Jw",
        "outputId": "6467a267-6151-4161-f97d-c07a60bb67a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating emission probabilities: 100%|██████████| 45872/45872 [00:40<00:00, 1138.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emission probabilities matrix:\n",
            "[[3.02102965e-01 0.00000000e+00 0.00000000e+00 ... 6.79544796e-05\n",
            "  7.24847782e-05 5.88938823e-05]\n",
            " [3.46889406e-04 5.10046100e-01 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.62245579e-02 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [2.13993146e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [8.81719026e-04 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.68968430e-03 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "E = np.zeros(shape=(N_TAGS, len(vocab)))\n",
        "\n",
        "for sentence in tqdm(train_data, desc=\"Calculating emission probabilities\"):\n",
        "    for word, tag in sentence:\n",
        "        word = word.lower()\n",
        "        if word not in vocab:\n",
        "            word = \"[UNK]\"\n",
        "        tag_index = list(POS_TAGS).index(tag)\n",
        "        word_index = vocab.index(word)\n",
        "        E[tag_index, word_index] += 1\n",
        "\n",
        "E /= E.sum(axis=1)[:, None]\n",
        "\n",
        "print(\"Emission probabilities matrix:\")\n",
        "print(E)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g3rwn64l5Jw"
      },
      "source": [
        "11. Using ``hmmlearn.hmm.CategoricalHMM``, create an HMM model ``pos_model`` for POS tagging. It will have POS tags as hidden states and English word tokens as outputs. Use the results from previous questions as parameters:\n",
        "  * Set `pos_model.startprob_` to `S`\n",
        "  * Set `pos_model.transmat_` to `P`\n",
        "  * Set `pos_model.emissionprob_` to `E`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "bdNdyhJcl5Jw"
      },
      "outputs": [],
      "source": [
        "from hmmlearn.hmm import CategoricalHMM\n",
        "\n",
        "pos_model = CategoricalHMM(n_components=N_TAGS)\n",
        "\n",
        "pos_model.startprob_ = S\n",
        "pos_model.transmat_ = P\n",
        "pos_model.emissionprob_ = E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjrAUhNTl5Jw"
      },
      "source": [
        "12. Write a function `convert_text_to_tokens(text)`\n",
        "    * This function receives a list of words\n",
        "    * It searches for each word in the text in the vocabulary you built and convert it to the corresponding\n",
        "    index in `vocab`\n",
        "    * return value is a numpy array of indices (tokens)\n",
        "    * *make sure to lowercase the word before searching in `vocab`*\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "tHrxoVECl5Jx"
      },
      "outputs": [],
      "source": [
        "def convert_text_to_tokens(text):\n",
        "    tokens = []\n",
        "    for word in text:\n",
        "        word = word.lower()\n",
        "        if word in vocab:\n",
        "            tokens.append(vocab.index(word))\n",
        "        else:\n",
        "            tokens.append(vocab.index(\"[UNK]\"))\n",
        "    return np.array(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i30jy1ll5Jx"
      },
      "source": [
        "13. Write a function `get_pos(sentence)` that returns the most likely POS tags for the words in the string ``sentence``.\n",
        "    * Convert the text to tokens using the function you wrote in the previous question\n",
        "    * Pass the tokens to `pos_model.decode(...)` to get the most likely POS tags.\n",
        "    \n",
        "    *Note: `pos_model.decode(...)` expects a 2D numpy array*\n",
        "    \n",
        "    Apply this to a few (lowercase, no punctuation) sentences including:\n",
        "    * \"this is a test\"\n",
        "    * \"tel aviv is in israel\"\n",
        "    * \"i know how to write code\"\n",
        "    \n",
        "    *split the sentences to words before calling your function*\n",
        "    \n",
        "    Do the results look reasonable to you?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg1EDo_Hl5Jx",
        "outputId": "e858ba4c-436d-4b9d-faf2-3484affb76a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: this is a test\n",
            "POS tags: ['DET', 'VERB', 'DET', 'NOUN']\n",
            "\n",
            "Sentence: tel aviv is in israel\n",
            "POS tags: ['NOUN', 'NOUN', 'VERB', 'ADP', 'NOUN']\n",
            "\n",
            "Sentence: i know how to write code\n",
            "POS tags: ['PRON', 'VERB', 'ADV', 'PRT', 'VERB', 'NOUN']\n",
            "\n",
            "Sentence: He is a smart boy\n",
            "POS tags: ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN']\n",
            "\n",
            "Sentence: She is the most beautiful in the room\n",
            "POS tags: ['PRON', 'VERB', 'DET', 'ADJ', 'ADJ', 'ADP', 'DET', 'NOUN']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def get_pos(sentence):\n",
        "    tokens = convert_text_to_tokens(sentence.split())\n",
        "    tokens = tokens.reshape(1, -1)\n",
        "    _ , predicted_states = pos_model.decode(tokens)\n",
        "    predicted_tags = [list(POS_TAGS)[i] for i in predicted_states]\n",
        "    return predicted_tags\n",
        "\n",
        "sentences = [\n",
        "    \"this is a test\",\n",
        "    \"tel aviv is in israel\",\n",
        "    \"i know how to write code\",\n",
        "    \"He is a smart boy\",\n",
        "    \"She is the most beautiful in the room\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    pos_tags = get_pos(sentence)\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"POS tags:\", pos_tags)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the results look reasonable."
      ],
      "metadata": {
        "id": "bMUADtXNyd8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(test_data[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JhZgB1ezFJr",
        "outputId": "c93853a2-50c1-4492-ab9b-0879084fa8c8"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Open', 'ADJ'), ('market', 'NOUN'), ('policy', 'NOUN')]\n",
            "[('And', 'CONJ'), ('you', 'PRON'), ('think', 'VERB'), ('you', 'PRON'), ('have', 'VERB'), ('language', 'NOUN'), ('problems', 'NOUN'), ('.', '.')]\n",
            "[('Mae', 'NOUN'), ('entered', 'VERB'), ('the', 'DET'), ('room', 'NOUN'), ('from', 'ADP'), ('the', 'DET'), ('hallway', 'NOUN'), ('to', 'ADP'), ('the', 'DET'), ('kitchen', 'NOUN'), ('.', '.')]\n",
            "[('This', 'DET'), ('will', 'VERB'), ('permit', 'VERB'), ('you', 'PRON'), ('to', 'PRT'), ('get', 'VERB'), ('a', 'DET'), ('rough', 'ADJ'), ('estimate', 'NOUN'), ('of', 'ADP'), ('how', 'ADV'), ('much', 'ADJ'), ('the', 'DET'), ('materials', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('shell', 'NOUN'), ('will', 'VERB'), ('cost', 'VERB'), ('.', '.')]\n",
            "[('the', 'DET'), ('multifigure', 'NOUN'), ('``', '.'), ('Traveling', 'VERB'), ('Carnival', 'NOUN'), (\"''\", '.'), (',', '.'), ('in', 'ADP'), ('which', 'DET'), ('action', 'NOUN'), ('is', 'VERB'), ('vivified', 'VERB'), ('by', 'ADP'), ('lighting', 'VERB'), (';', '.'), (';', '.')]\n",
            "[('They', 'PRON'), ('are', 'VERB'), ('in', 'ADP'), ('general', 'ADJ'), ('those', 'DET'), ('fears', 'NOUN'), ('that', 'PRON'), ('once', 'ADV'), ('seemed', 'VERB'), ('to', 'PRT'), ('have', 'VERB'), ('been', 'VERB'), ('amenable', 'ADJ'), ('to', 'ADP'), ('prayer', 'NOUN'), ('or', 'CONJ'), ('ritual', 'NOUN'), ('.', '.')]\n",
            "[('Yet', 'CONJ'), ('they', 'PRON'), ('are', 'VERB'), ('written', 'VERB'), (';', '.'), (';', '.')]\n",
            "[('The', 'DET'), ('plantation', 'NOUN'), ('was', 'VERB'), ('sold', 'VERB'), ('in', 'ADP'), ('January', 'NOUN'), (',', '.'), ('1845', 'NUM'), (',', '.'), ('and', 'CONJ'), ('Palfrey', 'NOUN'), ('thought', 'VERB'), ('the', 'DET'), ('new', 'ADJ'), ('owner', 'NOUN'), ('ought', 'VERB'), ('to', 'PRT'), ('pay', 'VERB'), ('his', 'DET'), ('people', 'NOUN'), ('two', 'NUM'), (\"months'\", 'NOUN'), ('wages', 'NOUN'), ('.', '.')]\n",
            "[('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('end', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('line', 'NOUN'), (\"''\", '.'), ('.', '.')]\n",
            "[('The', 'DET'), ('rules', 'NOUN'), ('and', 'CONJ'), ('policies', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('applied', 'VERB'), ('in', 'ADP'), ('this', 'DET'), ('process', 'NOUN'), ('of', 'ADP'), ('course', 'NOUN'), ('must', 'VERB'), ('be', 'VERB'), ('based', 'VERB'), ('on', 'ADP'), ('objectives', 'NOUN'), ('which', 'DET'), ('represent', 'VERB'), ('what', 'DET'), ('is', 'VERB'), ('to', 'PRT'), ('be', 'VERB'), ('desired', 'VERB'), ('if', 'ADP'), ('radio', 'NOUN'), ('service', 'NOUN'), ('is', 'VERB'), ('to', 'PRT'), ('be', 'VERB'), ('of', 'ADP'), ('maximum', 'ADJ'), ('use', 'NOUN'), ('to', 'ADP'), ('the', 'DET'), ('Nation', 'NOUN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7rTnFp3l5Jx"
      },
      "source": [
        "14. Split your test set to `X_test` (sentences) and `y_test` (tags)\n",
        "    Note: Both `X_test` and `y_test` should be lists of lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9JEwIKBl5Jx",
        "outputId": "59857431-b9cb-450b-e97b-a30e72ff8845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences in X_test: 11468\n",
            "Number of tags in y_test: 11468\n"
          ]
        }
      ],
      "source": [
        "X_test = [[word for word, _ in sentence] for sentence in test_data]\n",
        "y_test = [[tag for _, tag in sentence] for sentence in test_data]\n",
        "\n",
        "print(\"Number of sentences in X_test:\", len(X_test))\n",
        "print(\"Number of tags in y_test:\", len(y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(X_test[i], y_test[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-5MOsNhz_xN",
        "outputId": "8337b2e6-253e-440a-eb08-8dc23ab72542"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Open', 'market', 'policy'] ['ADJ', 'NOUN', 'NOUN']\n",
            "['And', 'you', 'think', 'you', 'have', 'language', 'problems', '.'] ['CONJ', 'PRON', 'VERB', 'PRON', 'VERB', 'NOUN', 'NOUN', '.']\n",
            "['Mae', 'entered', 'the', 'room', 'from', 'the', 'hallway', 'to', 'the', 'kitchen', '.'] ['NOUN', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n",
            "['This', 'will', 'permit', 'you', 'to', 'get', 'a', 'rough', 'estimate', 'of', 'how', 'much', 'the', 'materials', 'for', 'the', 'shell', 'will', 'cost', '.'] ['DET', 'VERB', 'VERB', 'PRON', 'PRT', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'ADV', 'ADJ', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', 'VERB', '.']\n",
            "['the', 'multifigure', '``', 'Traveling', 'Carnival', \"''\", ',', 'in', 'which', 'action', 'is', 'vivified', 'by', 'lighting', ';', ';'] ['DET', 'NOUN', '.', 'VERB', 'NOUN', '.', '.', 'ADP', 'DET', 'NOUN', 'VERB', 'VERB', 'ADP', 'VERB', '.', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQLbCcPel5Jx"
      },
      "source": [
        "15. Predict the POS tags of each sentence in the test set using the `get_pos` function you wrote\n",
        "</br>*Note: This step should take less than 10 seconds to run (use tqdm to track its progress).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaOQOfgLl5Jx",
        "outputId": "a26d3f23-327b-4616-a0cb-e61e3d68a005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting POS tags: 100%|██████████| 11468/11468 [00:12<00:00, 906.61it/s] \n"
          ]
        }
      ],
      "source": [
        "predicted_tags_all = []\n",
        "\n",
        "for sentence in tqdm(X_test, desc=\"Predicting POS tags\"):\n",
        "    sentence = \" \".join(sentence)\n",
        "    predicted_tags = get_pos(sentence)\n",
        "    predicted_tags_all.append(predicted_tags)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us8KtPrZl5Jx"
      },
      "source": [
        "16. Compute the accuracy of your predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txJjHI9Ll5Jx",
        "outputId": "9b57a75c-f131-448b-a4e3-683c3a0507ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.346180676665504\n"
          ]
        }
      ],
      "source": [
        "def compute_accuracy(predicted_tags_all, y_test):\n",
        "    total_sentences = len(predicted_tags_all)\n",
        "    correct_sentences = sum([1 for predicted_tags, true_tags in zip(predicted_tags_all, y_test) if predicted_tags == true_tags])\n",
        "    accuracy = correct_sentences / total_sentences\n",
        "    return accuracy\n",
        "\n",
        "accuracy = compute_accuracy(predicted_tags_all, y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZVhWsSv20uSH"
      },
      "execution_count": 79,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}