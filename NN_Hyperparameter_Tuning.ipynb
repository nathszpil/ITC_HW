{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is5bQ5JBml6X"
      },
      "source": [
        "# Neural Network Hyperparameter Tuning - Exercise\n",
        "\n",
        "In this exercise we will build a neural network to classify digits from the MNIST dataset, and explore how we may tune one of the model's hyperparameters to achieve better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHcFz3nkm0qF"
      },
      "source": [
        "## Part 1: Loading the dataset\n",
        "\n",
        "We will first load the MNIST data and prepare it for our model.\n",
        "\n",
        "**Questions:**\n",
        "1. Run the code given below to fetch the dataset.\n",
        "2. Examine the shapes of `X` and `y`. Explain in words what the 784 features in each row of `X` represent. (Hint: the images in MNIST are of size 28 x 28).\n",
        "3. Normalize the values of elements in `X` to be floats betweek `0.` and `1`, by dividing by a scalar value. Also cast values in `y` to be ints.\n",
        "4. Using `train_test_split` from sklearn, split the dataset into `X_train, X_test, y_train, y_test`. Use an 80-20 split. How many samples are in the train and test sets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP4uLAbXxKou",
        "outputId": "6dceba2f-618e-45e9-b981-4fd41f8163a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### CODE FOR QUESTION 1\n",
        "from sklearn.datasets import fetch_openml\n",
        "# Optinally, set data_home to where you want to download your data\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)\n",
        "\n",
        "X = X / 255.0\n",
        "y = y.astype(int)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the number of samples in train and test sets\n",
        "print(\"Number of samples in training set:\", len(X_train))\n",
        "print(\"Number of samples in test set:\", len(X_test))"
      ],
      "metadata": {
        "id": "yX-TrxDlLljI",
        "outputId": "a3d8c191-5fcb-4892-abb9-10198d80e456",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (70000, 784)\n",
            "Shape of y: (70000,)\n",
            "Number of samples in training set: 56000\n",
            "Number of samples in test set: 14000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row in X represents one image from the MNIST dataset.\n",
        "he dataset contains grayscale images that are 28x28 pixels.\n",
        "Each row in X has 28*28 = 784 features, where each feature represents the intensity of one pixel in the image."
      ],
      "metadata": {
        "id": "KC2pehKxLufb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3noy4nNFovC8"
      },
      "source": [
        "## Part 2: Building an MLP\n",
        "\n",
        "We will now build a neural network to classify the digits. We will use a simple MLP (\"multilayer perceptron\") model; this is also known as a \"vanilla\" feedforward neural network.\n",
        "\n",
        "An MLP is a sequential network consisting of multiple feedforward (Dense) layers with nonlinear activation functions.\n",
        "\n",
        "**Questions:**\n",
        "5. Using the imports given below, create a Keras sequential model called `model` to classify the digits. Use the following hints:\n",
        "  * The model should have a single hidden (Dense) layer with hidden dimension 50 and relu activation.\n",
        "  * The last layer of the model is also a Dense layer. Consider what its size and activation function should be, given that MNIST is a multiclass classification task with 10 classes (recall when we use sigmoid vs. softmax activations).\n",
        "  * Don't forget to use parameter `input_dim=...` for the first layer, since we are using the Keras Sequential API. Use a value that makes `model.input_shape` match the shapes of `X_train` and `X_test`.\n",
        "6. Print out `model.input_shape`, `model.output_shape`, and `model.summary()`. Take a look to make sure that what you see makes sense.\n",
        "7. How many parameters does the model have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epUF_xoZyUoY",
        "outputId": "459fd6e9-b1de-40c7-df33-f2b112ac05e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(50, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "print(\"Input shape:\", model.input_shape)\n",
        "print(\"Output shape:\", model.output_shape)\n",
        "model.summary()\n",
        "\n",
        "total_params = model.count_params()\n",
        "print(\"Total parameters:\", total_params)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (None, 784)\n",
            "Output shape: (None, 10)\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 50)                39250     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                510       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 39760 (155.31 KB)\n",
            "Trainable params: 39760 (155.31 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Total parameters: 39760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpcf3ZOIqokW"
      },
      "source": [
        "## Part 3: Training the model\n",
        "\n",
        "We will now train our model so that it learns to classify MNIST digits. We'll see that it performs much better on this task than the linear models we have seen before.\n",
        "\n",
        "**Questions:**\n",
        "8. Compile the model with `sparse_categorical_crossentropy` loss and `adam` optimizer. Also use parameter `metrics='accuracy'` so we can visualize the accuracy as the model trains.\n",
        "9. Train the model. In `model.fit(...)`, use parameters `validation_split=0.2` and `batch_size=16`. How can we tell how many epochs we should train the model for? (include your explanation of how your chose the number of epochs in your solution)\n",
        "10. What is the best validation loss and accuracy that your model achieved?\n",
        "\n",
        "**Note:** If you change something and want to train your model from scratch, make sure to re-run the code that created the model (`model = Sequential(...)`) to re-initialize its weights. Otherwise, `model.fit(...)` will continue from where you left off."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.2)\n",
        "\n",
        "\n",
        "best_val_loss = min(history.history['val_loss'])\n",
        "best_val_acc = max(history.history['val_accuracy'])\n",
        "\n",
        "print(\"Best validation loss:\", best_val_loss)\n",
        "print(\"Best validation accuracy:\", best_val_acc)"
      ],
      "metadata": {
        "id": "48VmiDlReJG0",
        "outputId": "9c452148-dbb6-4a4b-a88c-99f8aec39756",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2800/2800 [==============================] - 9s 3ms/step - loss: 0.3155 - accuracy: 0.9095 - val_loss: 0.1893 - val_accuracy: 0.9451\n",
            "Epoch 2/10\n",
            "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1554 - accuracy: 0.9542 - val_loss: 0.1544 - val_accuracy: 0.9547\n",
            "Epoch 3/10\n",
            "2800/2800 [==============================] - 7s 3ms/step - loss: 0.1141 - accuracy: 0.9656 - val_loss: 0.1280 - val_accuracy: 0.9620\n",
            "Epoch 4/10\n",
            "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0907 - accuracy: 0.9725 - val_loss: 0.1210 - val_accuracy: 0.9651\n",
            "Epoch 5/10\n",
            "2800/2800 [==============================] - 9s 3ms/step - loss: 0.0754 - accuracy: 0.9767 - val_loss: 0.1129 - val_accuracy: 0.9653\n",
            "Epoch 6/10\n",
            "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0633 - accuracy: 0.9808 - val_loss: 0.1147 - val_accuracy: 0.9664\n",
            "Epoch 7/10\n",
            "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0547 - accuracy: 0.9828 - val_loss: 0.1151 - val_accuracy: 0.9667\n",
            "Epoch 8/10\n",
            "2800/2800 [==============================] - 9s 3ms/step - loss: 0.0461 - accuracy: 0.9858 - val_loss: 0.1086 - val_accuracy: 0.9684\n",
            "Epoch 9/10\n",
            "2800/2800 [==============================] - 7s 3ms/step - loss: 0.0411 - accuracy: 0.9867 - val_loss: 0.1058 - val_accuracy: 0.9708\n",
            "Epoch 10/10\n",
            "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0349 - accuracy: 0.9892 - val_loss: 0.1221 - val_accuracy: 0.9677\n",
            "Best validation loss: 0.10578353703022003\n",
            "Best validation accuracy: 0.9708035588264465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I noticed that the validation accuracy improved consistently during the initial 10 epochs, but afterwards, it began to fluctuate. From this pattern, I concluded that training for 10 epochs was optimal."
      ],
      "metadata": {
        "id": "WaljgF6mecWo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g61hq7YfrtRZ"
      },
      "source": [
        "## Part 4: Hyperparameter Tuning\n",
        "\n",
        "In choosing our model we set a few hyperparameters, including the hidden layer dimension 50. It might have seemed like a \"magic number\". In fact, the best way to set hyperparameters like this is to perform a search using the validation set.\n",
        "\n",
        "For simplicity we will just try a few values for this single hyperparameter and see what gives the best model.\n",
        "\n",
        "**Questions:**\n",
        "11. Create new models `model20` and `model100`  with hidden layer dimensions of 20 and 100 respectively. Compile and train each model using the same procedure we used in part 3.\n",
        "12. Out of `20, 50, 100` which hidden layer dimension is best? Explain your answer, and store the best model in a new variable `best_model`.\n",
        "13. Using `best_model.evaluate(...)`, report the test set loss and accuracy of the model you chose in the previous question."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model20 = Sequential()\n",
        "model20.add(Dense(20, activation='relu', input_dim=X_train.shape[1]))\n",
        "model20.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model20.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "history20 = model20.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.2)"
      ],
      "metadata": {
        "id": "L3Wd8jSOgTpJ",
        "outputId": "0209420b-8121-4ca8-f2ea-166c736fb73e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2800/2800 [==============================] - 9s 3ms/step - loss: 0.3980 - accuracy: 0.8876 - val_loss: 0.2627 - val_accuracy: 0.9243\n",
            "Epoch 2/10\n",
            "2800/2800 [==============================] - 7s 2ms/step - loss: 0.2351 - accuracy: 0.9322 - val_loss: 0.2242 - val_accuracy: 0.9337\n",
            "Epoch 3/10\n",
            "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1973 - accuracy: 0.9427 - val_loss: 0.1977 - val_accuracy: 0.9411\n",
            "Epoch 4/10\n",
            "2800/2800 [==============================] - 7s 2ms/step - loss: 0.1747 - accuracy: 0.9476 - val_loss: 0.1890 - val_accuracy: 0.9438\n",
            "Epoch 5/10\n",
            "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1574 - accuracy: 0.9531 - val_loss: 0.1766 - val_accuracy: 0.9473\n",
            "Epoch 6/10\n",
            "2800/2800 [==============================] - 7s 2ms/step - loss: 0.1433 - accuracy: 0.9563 - val_loss: 0.1802 - val_accuracy: 0.9467\n",
            "Epoch 7/10\n",
            "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1324 - accuracy: 0.9592 - val_loss: 0.1661 - val_accuracy: 0.9508\n",
            "Epoch 8/10\n",
            "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1233 - accuracy: 0.9619 - val_loss: 0.1686 - val_accuracy: 0.9505\n",
            "Epoch 9/10\n",
            "2800/2800 [==============================] - 7s 2ms/step - loss: 0.1156 - accuracy: 0.9649 - val_loss: 0.1673 - val_accuracy: 0.9496\n",
            "Epoch 10/10\n",
            "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1097 - accuracy: 0.9661 - val_loss: 0.1580 - val_accuracy: 0.9545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = min(history20.history['val_loss'])\n",
        "best_val_acc = max(history20.history['val_accuracy'])\n",
        "\n",
        "print(\"Best validation loss:\", best_val_loss)\n",
        "print(\"Best validation accuracy:\", best_val_acc)"
      ],
      "metadata": {
        "id": "yrK_4OqHh8IL",
        "outputId": "2c40851b-01f6-4e99-a9b4-d4570f567f7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best validation loss: 0.15803465247154236\n",
            "Best validation accuracy: 0.9544642567634583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model100 = Sequential()\n",
        "model100.add(Dense(100, activation='relu', input_dim=X_train.shape[1]))\n",
        "model100.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model100.compile(optimizer='adam',\n",
        "                 loss='sparse_categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "history100 = model100.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.2)\n",
        "\n",
        "best_val_loss = min(history100.history['val_loss'])\n",
        "best_val_acc = max(history100.history['val_accuracy'])\n",
        "\n",
        "print(\"Best validation loss:\", best_val_loss)\n",
        "print(\"Best validation accuracy:\", best_val_acc)"
      ],
      "metadata": {
        "id": "MK_701gmgah8",
        "outputId": "46f43e9a-f0c5-481e-88b1-1640ac5b8835",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2800/2800 [==============================] - 10s 3ms/step - loss: 0.2690 - accuracy: 0.9222 - val_loss: 0.1685 - val_accuracy: 0.9499\n",
            "Epoch 2/10\n",
            "2800/2800 [==============================] - 11s 4ms/step - loss: 0.1198 - accuracy: 0.9649 - val_loss: 0.1181 - val_accuracy: 0.9658\n",
            "Epoch 3/10\n",
            "2800/2800 [==============================] - 11s 4ms/step - loss: 0.0820 - accuracy: 0.9750 - val_loss: 0.1004 - val_accuracy: 0.9708\n",
            "Epoch 4/10\n",
            "2800/2800 [==============================] - 10s 4ms/step - loss: 0.0592 - accuracy: 0.9815 - val_loss: 0.0960 - val_accuracy: 0.9710\n",
            "Epoch 5/10\n",
            "2800/2800 [==============================] - 10s 4ms/step - loss: 0.0450 - accuracy: 0.9864 - val_loss: 0.0857 - val_accuracy: 0.9732\n",
            "Epoch 6/10\n",
            "2800/2800 [==============================] - 9s 3ms/step - loss: 0.0358 - accuracy: 0.9891 - val_loss: 0.0878 - val_accuracy: 0.9752\n",
            "Epoch 7/10\n",
            "2800/2800 [==============================] - 10s 4ms/step - loss: 0.0292 - accuracy: 0.9905 - val_loss: 0.0975 - val_accuracy: 0.9737\n",
            "Epoch 8/10\n",
            "2800/2800 [==============================] - 11s 4ms/step - loss: 0.0221 - accuracy: 0.9928 - val_loss: 0.0927 - val_accuracy: 0.9751\n",
            "Epoch 9/10\n",
            "2800/2800 [==============================] - 11s 4ms/step - loss: 0.0202 - accuracy: 0.9937 - val_loss: 0.0997 - val_accuracy: 0.9755\n",
            "Epoch 10/10\n",
            "2800/2800 [==============================] - 11s 4ms/step - loss: 0.0166 - accuracy: 0.9946 - val_loss: 0.0998 - val_accuracy: 0.9756\n",
            "Best validation loss: 0.0857173502445221\n",
            "Best validation accuracy: 0.9756249785423279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to train each model for 10 epochs, we can  see that the best model is the model with 100 hidden layers, we get to the best validation accuracy and loss for this one(although it is not much different than 50 layers in term of performances)."
      ],
      "metadata": {
        "id": "TyjUf6s-iTpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = model100\n",
        "\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
        "print(\"Test set loss:\", test_loss)\n",
        "print(\"Test set accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "tmxzydIgiMmG",
        "outputId": "b6481f9d-9c0d-4159-d1a9-0526b45fd5b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "438/438 [==============================] - 1s 2ms/step - loss: 0.1040 - accuracy: 0.9739\n",
            "Test set loss: 0.10400651395320892\n",
            "Test set accuracy: 0.9738571643829346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CCM3c6kTi2If"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}