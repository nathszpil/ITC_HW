{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIdz5muhrOUh"
   },
   "source": [
    "<font size=\"36\"><b>Sklearn Pipelines - Self Study - Assignment</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMgD_nA9rOUn"
   },
   "source": [
    "In this exercise we will work with the Automobile dataset from <a href = \"https://archive.ics.uci.edu/ml/datasets/Automobile\">UCI</a>. We revised it for your comfort, so please use the attached files.\n",
    "\n",
    "We will try to predict the automobile **price**\n",
    "\n",
    "The data dictionary is attached (`imports-85.names` file).\n",
    "\n",
    "Explanation of some other columns (see also `imports-85.names` file):\n",
    "- **symboling** - Risk rating.  Corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process \"symboling\". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.\n",
    "- The **normalized-losses** is relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc...), and represents the average loss per car per year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG_GFe_FrOUp"
   },
   "source": [
    "# Load the dataset and perform initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7qJ6cCwrOUq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Automobile.csv\")\n",
    "\n",
    "print(\"First few rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nMissing values in the dataset:\")\n",
    "display(df.isnull().sum())\n",
    "\n",
    "print(\"\\nSummary statistics of the dataset:\")\n",
    "display(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG2g_grtrOUt"
   },
   "source": [
    "# Do initial critical transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwSuDFAbrOUu"
   },
   "source": [
    "Check your **target** variable, and remove samples that will not allow us to train and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ha3rhfurOUu"
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=['price'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFawnGzQrOUv"
   },
   "source": [
    "# Split the data set to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4q0CFD-orOUw"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bESBEca2rOUx"
   },
   "source": [
    "# Split the features into different types\n",
    "Split into different data types.\n",
    "\n",
    "It will help you to do EDA on it separately and it preprocess separately.\n",
    "\n",
    "**Hint:** Use the attached data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvU4IahVrOUx"
   },
   "outputs": [],
   "source": [
    "file_path = \"imports-85.names\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    file_contents = file.read()\n",
    "\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJbsWYE7rOUy"
   },
   "source": [
    "# Perform more LIGHT EDA \n",
    "- This is to help decide how to preprocess the data.\n",
    "- Focus on **minimal** things that will help you do encoding, and handling NaNs.\n",
    "- **No need** to understand correlations between features, and between features and target variable, etc.\n",
    "\n",
    "**Warning:** Don't overdo it, this exercise is about learning pipelines, not about EDA.\n",
    "\n",
    "**Important hint**: What feature has almost always the same value?  It's import to recognize it and remove it later.  Otherwise it can cause a lot of issues, especially when transforming with Cross-validation, since one of the values will often not be found, and can give us a lot of problems with preprocessing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70wAQ1O4rOUz"
   },
   "outputs": [],
   "source": [
    "# Identify categorical and numerical features for encoding\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = df.select_dtypes(exclude=['object']).columns.tolist()\n",
    "numerical_features.remove('price')\n",
    "\n",
    "print(\"\\nCategorical features:\")\n",
    "print(categorical_features)\n",
    "print(\"\\nNumerical features:\")\n",
    "print(numerical_features)\n",
    "\n",
    "# Check the value counts for categorical features\n",
    "for column in categorical_features:\n",
    "    print(\"\\nValue counts for\", column, \":\")\n",
    "    print(df[column].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that engine-location is a redundant feature so we will drop it in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEw5i7NJrOUz"
   },
   "source": [
    "# Define pipeline logical steps\n",
    "With words and explanations, define the specific steps you want to take as a part of your pipeline and explain the reason for each.\n",
    "\n",
    "**Hints:**\n",
    "- Think **not** about specific columns, but how to do as similar as possible steps on **multiple** columns using built-in transformers\n",
    "- To keep this exercise simple, it's **OK to make sub-optimal preprocessing**, as long as it's reasonable.  For example, you don't need to change distribution shapes of continuous features\n",
    "\n",
    "**You MUST include:**\n",
    "* NA care\n",
    "* Removing 1 problematic feature discussed in EDA step above\n",
    "* Categorical feature encoding\n",
    "* Data Normalization\n",
    "* Feature selection / dimensionaliry reduction\n",
    "* Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cJIuVfzrOU0"
   },
   "source": [
    "    1. NA Care:\n",
    "        Impute missing values: We will fill in missing values in numerical features with mean. For categorical features, we will use a separate category for missing values.\n",
    "\n",
    "    2. Removing 1 Problematic Feature:\n",
    "        Remove feature with almost always the same value: we will eliminate 'engine-location' since it was identified as redundant during EDA.\n",
    "\n",
    "    3. Categorical Feature Encoding:\n",
    "        One-Hot Encoding or Ordinal Encoding: Convert categorical features into numerical format using one-hot encoding for nominal variables and ordinal encoding for ordinal variables.\n",
    "\n",
    "    4. Data Normalization:\n",
    "        Standardization or Min-Max Scaling: Scale numerical features to a similar scale using standardization or min-max scaling.\n",
    "\n",
    "    5. Feature Selection / Dimensionality Reduction:\n",
    "        SelectKBest: Reduce dimensionality by selecting top k features based on statistical tests (e.g., chi-square, ANOVA) to improve model performance and reduce overfitting.\n",
    "\n",
    "    6. Modeling:\n",
    "        Choose a regression algorithm: Train a regression model using selected features. We will use Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xzNfZJUrOU1"
   },
   "source": [
    "# Implement a pipeline \n",
    "Include all of the steps you mentioned above.\n",
    "\n",
    "**Hints:**\n",
    "- You can make some changes to your decisions to make the pipeline simpler, but explain all changes, steps and decisions.\n",
    "- Notice that by default `ColumnTransformer` will drop all features that were not explicitly given to it in one of the transformers.  This is one way to always drop some columns\n",
    "- If you are having issues running the pipeline, try to debug parts of the pipeline and the it's outputs\n",
    "- If you are having issues, make sure you understand what `handle_unknown` parameter options do for various transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features.remove('engine-location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bN0zGiRcrOU2"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n",
    "                                        ('scaler', StandardScaler()),\n",
    "                                        ('pca', PCA (n_components=3))\n",
    "                                       ])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer (strategy='most_frequent')),\n",
    "                                          ('onehot', OneHotEncoder (handle_unknown='ignore'))\n",
    "                                         ])\n",
    "\n",
    "# Create a preprocessor to handle both numerical and categorical features \n",
    "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_features), \n",
    "                                               ('cat', categorical_transformer, categorical_features)\n",
    "                                              ])\n",
    "\n",
    "# Create the final pipeline with the preprocessor and the model\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "                           ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djYWSEmlrOU3"
   },
   "source": [
    "# Use the pipeline\n",
    "- Fit the pipeline\n",
    "- Evaluate the model recieved. Are you satisfied with your score? \n",
    "- Print your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Evaluation metrics:\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "print(\"\\nPipeline:\")\n",
    "print(pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not so satisfied with the results, even if we obtain a relatively good R2 score the mse and mae are quite big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3n6RKkQCrOU4"
   },
   "source": [
    "# Model selection / hyperparam tuning\n",
    "- Try a few different options for preprocessing and/or modeling, that you think has a good chance to improve the metric of the final model.  Use `RandomizedSearch`\n",
    "- Is the score better now?\n",
    "- Print the pipeline chosen by the search\n",
    "- Print the best hyperparameters of the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define different preprocessing options\n",
    "preprocessing_options = [\n",
    "    {\n",
    "        'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "        'preprocessor__num__pca__n_components': [3, 5, 7],\n",
    "        'preprocessor__cat__imputer__strategy': ['most_frequent', 'constant'],\n",
    "        'preprocessor__cat__onehot__handle_unknown': ['error', 'ignore']\n",
    "    },\n",
    "    {\n",
    "        'preprocessor__num__imputer__strategy': ['mean'],\n",
    "        'preprocessor__num__pca__n_components': [3,4,5,8],\n",
    "        'preprocessor__cat__imputer__strategy': ['most_frequent'],\n",
    "        'preprocessor__cat__onehot__handle_unknown': ['ignore']\n",
    "    }\n",
    "]\n",
    "\n",
    "param_grid = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__max_depth': [None, 10, 20],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,  # Number of parameter settings that are sampled\n",
    "    scoring='neg_mean_squared_error',  # Evaluation metric: mean squared error\n",
    "    cv=5,  # Cross-validation folds\n",
    "    random_state=42,\n",
    "    verbose=2  # Verbosity level\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best score:\", random_search.best_score_)\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "\n",
    "print(\"Best pipeline:\", random_search.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
